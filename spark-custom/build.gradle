apply plugin: 'base'

def sparkVersion = "2.4.3"
def scalaVersion = "2.12"
def scalaVersionFull = "2.12.8"


task process(){
   doFirst {
   	println("Starting production of artifacts by Suresh Pillay...")
   }
   doLast {
   	println("Suresh Pillay...complete")
   }
}

task processInit(){
   doLast {
   	println("Initialized Structures...")
   }
}
task processDev(){
   doFirst {
   	println("Starting production of eclipse artifacts by Suresh Pillay...")
   }
   doLast {
   	println("Suresh Pillay...now you ready to import into eclipse")
   }
}

task copyAll(){
   doLast {
   	println("Copying src files...")
   }
}
task backup(){
   doLast {
   	println("backup files...")
   }
}

clean.doLast {
    file("v${sparkVersion}.tar.gz").delete()
    file("spark-${sparkVersion}").deleteDir()
    file('pax_global_header').delete()
}


task pullSpark(type:Exec){
   commandLine 'wget',"https://github.com/apache/spark/archive/v${sparkVersion}.tar.gz"
}

task extractSpark(type: Copy) {
   from tarTree(resources.gzip("v${sparkVersion}.tar.gz"))
   into getProjectDir()
}


task copySrcA(type: Copy) {
    from "src/SPFunctions.scala"
    into "spark-${sparkVersion}/sql/core/src/main/scala/org/apache/spark/sql/"
}
task copySrcB(type: Copy) {
    from "src/SPEmo.scala"
    into "spark-${sparkVersion}/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/"
}
task copySrcC(type: Copy) {
    from "src/SPUDFSuite.scala"
    into "spark-${sparkVersion}/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/"
}
task bkSrcA(type: Copy) {
    from "spark-${sparkVersion}/sql/core/src/main/scala/org/apache/spark/sql/SPFunctions.scala"
    into "src/"
}
task bkSrcB(type: Copy) {
    from "spark-${sparkVersion}/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SPEmo.scala"
    into "src/"
}
task bkSrcC(type: Copy) {
    from "spark-${sparkVersion}/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SPUDFSuite.scala"
    into "src/"
}

task scalaSpark(type:Exec){
  workingDir "spark-${sparkVersion}"
  commandLine './dev/change-scala-version.sh',"${scalaVersion}"
}

task cleanSpark(type:Exec){
  workingDir "spark-${sparkVersion}"
  commandLine './build/sbt',"-Dscala.version=${scalaVersionFull}",'clean'
}

task compileSpark(type:Exec){
  workingDir "spark-${sparkVersion}"
  commandLine './build/sbt',"-Dscala.version=${scalaVersionFull}",'compile'
}
task compileCat(type:Exec){
  workingDir "spark-${sparkVersion}"
  commandLine './build/sbt',"-Dscala.version=${scalaVersionFull}",'project catalyst','compile'
}
task compileSQL(type:Exec){
  workingDir "spark-${sparkVersion}"
  commandLine './build/sbt',"-Dscala.version=${scalaVersionFull}",'project sql','compile'
}

task publishSpark(type:Exec){
  workingDir "spark-${sparkVersion}"
  commandLine './build/sbt',"-Dscala.version=${scalaVersionFull}",'publish-local'
}
task eclipseSpark(type:Exec){
  workingDir "spark-${sparkVersion}"
  commandLine './build/sbt',"-Dscala.version=${scalaVersionFull}",'eclipse'
}



copyAll.dependsOn copySrcA
copyAll.dependsOn copySrcB
copyAll.dependsOn copySrcC
//publishSpark.dependsOn compileSpark
publishSpark.dependsOn compileSQL
compileSQL.dependsOn compileCat
compileSpark.dependsOn cleanSpark
//cleanSpark.dependsOn scalaSpark
//scalaSpark.dependsOn copyAll
// Starting point
process.dependsOn publishSpark
// Initialization starting point is processInit
extractSpark.dependsOn pullSpark
processInit.dependsOn extractSpark
// prep for dev
processDev.dependsOn eclipseSpark
// backup
backup.dependsOn bkSrcC
bkSrcB.dependsOn bkSrcA
bkSrcC.dependsOn bkSrcB
